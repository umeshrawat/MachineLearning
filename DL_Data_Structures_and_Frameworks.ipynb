{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umeshrawat/MachineLearning/blob/master/DL_Data_Structures_and_Frameworks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: In this coding question, you are tasked with building a neural network with three layers using PyTorch. You need to fill in the blanks for the activation function, initialization, and the forward function to complete the implementation. The provided testing code allows them to check whether their neural network is functioning as expected."
      ],
      "metadata": {
        "id": "wMLYXSdhjfQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Step 1: Layer Dimensions\n",
        "input_dim = 5  # Fill in the blank for the number of input features\n",
        "hidden_dim = 10  # Fill in the blank for the number of hidden units\n",
        "output_dim = 3  # Fill in the blank for the number of output units\n",
        "\n",
        "# Step 2: Activation Function\n",
        "def activation_function(x):\n",
        "    return torch.relu(x)  # Fill in the blank for the activation function\n",
        "\n",
        "# Step 3: Neural Network Architecture\n",
        "class ThreeLayerNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ThreeLayerNN, self).__init__()\n",
        "        self.input_layer = nn.Linear(input_dim , hidden_dim)  # Fill in the blanks for input layer dimensions\n",
        "        self.hidden_layer = nn.Linear(hidden_dim , hidden_dim)  # Fill in the blanks for hidden layer dimensions\n",
        "        self.output_layer = nn.Linear(hidden_dim , output_dim)  # Fill in the blanks for output layer dimensions\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 10 by 1\n",
        "        hidden_output = activation_function(self.input_layer(x))  # Fill in the blank for the forward pass through the input layer\n",
        "\n",
        "        hidden_output = activation_function(self.hidden_layer(hidden_output))  # Fill in the blank for the forward pass through the hidden layer\n",
        "        # 3 by 1\n",
        "        output = self.output_layer(hidden_output)  # Fill in the blank for the forward pass through the output layer\n",
        "\n",
        "        return torch.softmax(output, dim = 1)\n",
        "\n",
        "# Step 4: Initialization and Testing\n",
        "x = torch.randn(1, input_dim)\n",
        "model = ThreeLayerNN()\n",
        "output = model(x)\n",
        "print(\"Output tensor:\", output)\n"
      ],
      "metadata": {
        "id": "-etFclfcjgRv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41b1a020-a8aa-4512-a3f8-17b62ce62f32"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output tensor: tensor([[0.2674, 0.3722, 0.3604]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: Your task is to fill in the blanks in the \"Define the Real Function\" section to complete the implementation of the real function f(x). Then, fill in the blanks in the \"Compute the Gradient\" section to compute the gradient of the real function with respect to the input x. Lastly, obtain and print the gradient of x."
      ],
      "metadata": {
        "id": "8iXVgEjGkqP1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the real function f(x) = (x^4 - 3x^3 + 2x^2 - 5x + 7) / (2x^3 + 5x^2 - x + 3)\n",
        "def real_function(x):\n",
        "    result = (x**4 - 3*x**3 + 2*x**2 - 5*x + 7) / (2*x**3 + 5*x**2 - x + 3)  # Fill in the blank for the computation of the function\n",
        "    return result\n",
        "\n",
        "# Define a real number for which to compute the gradient\n",
        "x = torch.tensor(1.0, requires_grad=True, dtype=torch.float32)\n",
        "\n",
        "# Compute the real function f(x)\n",
        "result = real_function(x)  # Fill in the blank to call the real function with input 'x'\n",
        "\n",
        "# Compute the gradient of f(x) with respect to x\n",
        "result.backward()  # Fill in the blank to compute the gradient of 'result'\n",
        "\n",
        "# Obtain the gradient of x\n",
        "gradient_x = x.grad  # Fill in the blank for the gradient of 'x'\n",
        "\n",
        "print(\"Gradient of x:\", gradient_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3XEdmJBkqzk",
        "outputId": "26d6ad0b-73d3-4eb8-8dc6-e7fdfe9a0141"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient of x: tensor(-1.0370)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional Question: Below is an incomplete code for Logistic Regression in Pytorch. Your task is to fill the appropriate function calls and run the for teh specified number of epochs."
      ],
      "metadata": {
        "id": "5a7EZdZy4ItW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "# Define the number of features (input size)\n",
        "input_size = 2\n",
        "\n",
        "# Define the number of classes (output size)\n",
        "output_size = 2\n",
        "\n",
        "# Create the training dataset\n",
        "X_train = torch.tensor([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0], [5.0, 1.0]])\n",
        "y_train = torch.tensor([0, 0, 1, 1])  # Fill in the blanks for the appropriate y_train values\n",
        "\n",
        "\n",
        "# Step 2: Define the logistic regression model\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.linear(x))\n",
        "\n",
        "model = LogisticRegressionModel()\n",
        "\n",
        "# Step 3: Define the loss function and optimizer\n",
        "criterion = nn.BCELoss()  # Fill in the blank for the appropriate loss function\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Fill in the blank for the appropriate optimizer\n",
        "\n",
        "# Step 4: Train the model\n",
        "num_epochs = 1000\n",
        "for epoch in range(num_epochs):\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)  # Fill in the blanks for the appropriate y_train values\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss = criterion(torch.max(outputs, dim = 1).values, y_train.float())  # Fill in the blanks for the appropriate y_train values\n",
        "    optimizer.___()\n",
        "\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Step 5: Test the model\n",
        "X_test = torch.tensor([[4.0, 3.0]])\n",
        "predicted = model(X_test)\n",
        "predicted_class = torch.argmax(___).item()\n",
        "print(\"Predicted class:\", predicted_class)  # Fill in the blank for the appropriate tensor to get the predicted class"
      ],
      "metadata": {
        "id": "Gqmgx06BvoH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional Question: Implementing Single-Headed Attention with einsum\n",
        "\n",
        "In this coding question, you will implement a single-headed attention function using einsum in PyTorch. The provided inspiration function demonstrates the computation of single-headed attention without using einsum. Your task is to take inspiration from the given function and write a new function that performs the same computation using einsum.\n",
        "Explanation of the code without Einsum:\n",
        "Inputs:\n",
        "\n",
        "query: A PyTorch tensor of shape (batch_size, sequence_length, query_dim). This tensor represents the query vectors for each token in the sequence.\n",
        "key: A PyTorch tensor of shape (batch_size, sequence_length, key_dim). This tensor represents the key vectors for each token in the sequence.\n",
        "value: A PyTorch tensor of shape (batch_size, sequence_length, value_dim). This tensor represents the value vectors for each token in the sequence.\n",
        "Outputs:\n",
        "\n",
        "attended_values: A PyTorch tensor of shape (batch_size, sequence_length, value_dim). This tensor represents the attended values obtained by combining the value vectors with the attention weights.\n",
        "attention_weights: A PyTorch tensor of shape (batch_size, sequence_length, sequence_length). This tensor contains the attention weights indicating the importance of each token's value in producing the attended values."
      ],
      "metadata": {
        "id": "mvZKas4YwFbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Single headed attention without EinSum\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def single_head_attention(query, key, value):\n",
        "    # Calculate the dot product between query and key\n",
        "    dot_product = torch.matmul(query, key.transpose(-2, -1))\n",
        "\n",
        "    # Scale the dot product by dividing it by the square root of the dimension of the key vector\n",
        "    scaled_dot_product = dot_product / (key.size(-1) ** 0.5)\n",
        "\n",
        "    # Apply softmax to obtain attention weights along the last dimension (head dimension)\n",
        "    attention_weights = F.softmax(scaled_dot_product, dim=-1)\n",
        "\n",
        "    # Compute the weighted sum of value vectors\n",
        "    attended_values = torch.matmul(attention_weights, value)\n",
        "\n",
        "    return attended_values, attention_weights\n",
        "\n",
        "\n",
        "# Einsum based attention goes here\n",
        "def single_head_attention_with_einsum(query, key, value):\n",
        "  # Code goes here\n",
        "  pass"
      ],
      "metadata": {
        "id": "TYf8PTaIwGHJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}